{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cursory view of data\n",
    "# with open ('data/adult.names') as fin:\n",
    "#     notes = fin.read()\n",
    "# print(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta = pd.read_csv('data/adult.data.cleaned.csv.gz', compression = 'gzip')\n",
    "test = pd.read_csv('data/adult.test.cleaned.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORING THE DATA\n",
    "\n",
    "# import pandas_profiling as pp ; pp.ProfileReport(df)\n",
    "\n",
    "#df.sample(5)\n",
    "#df.head()\n",
    "#df.tail()\n",
    "#df.info()\n",
    "#df.describe(include = 'all')\n",
    "#df.min() #helps detect negative values!\n",
    "#df.shape\n",
    "#df.column.unique()\n",
    "#pd.set_option('display.max_columns', 100)\n",
    "#df['Name'][df['Opportunity value'] > 70].describe()\n",
    "#df.col.value_counts()#(normalize=True).plot()\n",
    "##pd.plotting.scatter_matrix(df.head(20), figsize = (15,5), diagonal = 'hist' )\n",
    "#plt.xticks(rotation = 90)\n",
    "#df.col.value_counts().count\n",
    "#df.sort_values(by='column')\n",
    "#df[df['Events'].isin(['Rain','Snow'])]\n",
    "\n",
    "\n",
    "\n",
    "###VISUALIZING HISTOGRAM FOR EACH COLUMN WITH NUM VALUES #warning: time-consuming## \n",
    "# df.hist(bins = 15, figsize = (20,15))\n",
    "# plt.show()\n",
    "\n",
    "#VISUALIZING BOTH CATEGORICAL AND NUMERICAL DATA:\n",
    "# #explore the data graphically\n",
    "# fig = plt.figure(figsize = (20,20))\n",
    "# cols = 3\n",
    "# rows = math.ceil(float(df.shape[1] / cols))\n",
    "\n",
    "# for i, column in enumerate(list(df.columns)):\n",
    "#     ax = fig.add_subplot(rows, cols, i + 1)\n",
    "#     ax.set_title(column)\n",
    "#     if df.dtypes[column] == np.object:\n",
    "#         df[column].value_counts().plot(kind = 'bar', axes = ax)\n",
    "#     else:\n",
    "#         df[column].hist(axes = ax)\n",
    "#         plt.xticks(rotation = 'vertical')\n",
    "# plt.subplots_adjust(hspace = 0.7, wspace = 0.2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSPECTING FOR WRONG DATA, DUPLICATES OR OUTLIERS\n",
    "#df.isnull().values.any()\n",
    "#df.isna().sum() #COUNT MISSING VALUES PER COLUMN\n",
    "#df.isnull().any()\n",
    "#CREATE GRAPH FOR NULL VALUES: sns.heatmap(df.isnull(),yticklabels = False, cbar = False, cmap = 'viridis')\n",
    "#df[(df == '?').any(axis=1)].count() #any row with question mark\n",
    "\n",
    "#total_null = df.isna().sum().sort_values(ascending = False)\n",
    "#percent = (df.isna().sum() / df.isna().count()).sort_values(ascending = False)\n",
    "#missing_data = pd.concat([total_null, percent], axis = 1, keys = ['Total', 'Percent'])\n",
    "\n",
    "#df.col.duplicated().value_counts()\n",
    "#df.duplicated().sum()\n",
    "#df.drop_duplicates(keep = 'first', inplace = True)\n",
    "#df.loc[:, ['col1', 'col2']]\n",
    "\n",
    "#Generate new column with missing data from another column\n",
    "# df['NaN'] = df['col'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING AND REARRANGING THE DATA\n",
    "\n",
    "#df.dropna()\n",
    "#df.col.fillna(0)\n",
    "#df.col.fillna(df['col'].mean())\n",
    "#df.rename(columns = {'col1':'col1name'})\n",
    "#df.columns.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING THE DATA TO VISUALIZE ANOMALIES\n",
    "\n",
    "#df.plot(figsize = (15,5))\n",
    "#df.marital_status.value_counts().plot(kind = 'bar')\n",
    "#plt.hist(df.age, bins = 20)\n",
    "#plt.scatter(df.occupation, df.age)\n",
    "#plt.plot(df.capital_gain)\n",
    "#SHOWING TITLES\n",
    "# plt.title = ('Distribution by age')\n",
    "# plt.xlabel = ('Player name')\n",
    "# plt.ylabel = ('Age')\n",
    "\n",
    "#PLOTTING BY GROUP\n",
    "# test = df.groupby('auction_type')\n",
    "# test.get_group('5 day auction')[['price','openbid']].plot(figsize = (15,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZE CORRELATIONS\n",
    "\n",
    "#df.corr(method = 'spearman') #other methods are 'kendall' and 'pearson'\n",
    "#df['age'].corr(df['capital_gain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##VISUALIZE CORRELATIONS GRAPHICALLY\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "####SIMPLE REGRESSION LINE PLOT####\n",
    "#sns.pairplot(df, x_vars = ['capital_loss', 'hours_per_week', 'education_num'], y_vars = 'age', size = 7, aspect = 0.7, kind = 'reg')\n",
    "\n",
    "####MULTI CORR SEPARATED####\n",
    "sns.pairplot(data = df)\n",
    "\n",
    "#Distplot\n",
    "#sns.distplot(df['column'])\n",
    "\n",
    "#SIMPLE CORR\n",
    "sns.heatmap(df.corr())\n",
    "\n",
    "#GRAPH 1 CORR\n",
    "# # Compute the correlation matrix\n",
    "# corr = df.corr()\n",
    "# # Set up the matplotlib figure\n",
    "# f, ax = plt.subplots(figsize=(15, 10))\n",
    "# # Draw the heatmap with the mask and correct aspect ratio\n",
    "# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "#             square=True, linewidths=.5)\n",
    "\n",
    "####GRAPH 2 CORR HEATMAP####\n",
    "# corr = df.corr()\n",
    "# corr = (corr)\n",
    "# sns.heatmap(corr,\n",
    "#            xticklabels = corr.age.values,\n",
    "#            yticklabels = corr.capital_gain.values)\n",
    "\n",
    "####GRAPH 3 BOXPLOT####\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# fig = sns.boxplot(x = df.marital_status, y = df.age)\n",
    "\n",
    "#GRAPH 4 SIMPLE HORIZONTAL\n",
    "# f, ax = plt.subplots(figsize=(15, 5))\n",
    "# sns.countplot(y='col1', hue='col2', data=df).set_title('Title')\n",
    "\n",
    "#GRAPH 5 SEPARATED BOXPLOT\n",
    "# df.plot(kind = 'box', subplots = True, layout = (2,2), sharex = False, sharey = False)\n",
    "# plt.show()\n",
    "\n",
    "####GRAPH 6 SEPARATED HIST####\n",
    "# df.hist(figsize=(20, 20))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MANIPULATING THE DATA TO VIEW IT FROM DIFFERENT ANGLES\n",
    "\n",
    "#df.sort_values(['col1','col2'], ascending = False).head(20)\n",
    "#df.groupby(df.col).count().plot(kind = 'bar')\n",
    "#df.groupby('col1')['col2'].mean().sort_values(ascending = False).head(10)\n",
    "# test = df.groupby('auction_type') # print(test.get_group('3 day auction').describe())\n",
    "#pd.crosstab(df.col1, df.col2).apply(lambda x: x/x.sum(), axis=1)\n",
    "#df['young_male'] = ((df.gender == 'M') & (df.age < 30)).map({True: 'young male', False: 'other'})\n",
    "#df.pivot_table(index = 'occupation', values = ['binary_gender', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPLACING VALUES\n",
    "\n",
    "#DROP multiple columns\n",
    "#df.drop(['sex_enc' , 'enc_country'], axis = 1, inplace = True)\n",
    "\n",
    "#Replace with regex\n",
    "# df = df.replace({\n",
    "#     'Value':'[A-Za-z]', \n",
    "#     'Wage': '[A-Za-z]',\n",
    "# },'',regex = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPLYING FORMULAE\n",
    "\n",
    "# def euro(cell):\n",
    "#     cell = cell.strip('€')\n",
    "#     return cell\n",
    "# df.Wage = df.Wage.apply(euro)\n",
    "\n",
    "\n",
    "#Insert value in cell depending on values from other cells\n",
    "# def impute_age(cols):\n",
    "#     age = cols[0]\n",
    "#     Pclass = cols[1]\n",
    "#     if pd.isnull(Age):\n",
    "#         if Pclass == 1:\n",
    "#             return 37\n",
    "#         elif Pclass == 2:\n",
    "#             return 29\n",
    "#         else:\n",
    "#             return 24\n",
    "#     else:\n",
    "#         return Age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGING DATA TYPES\n",
    "#changing values to float\n",
    "#df[['Value','Wage','Age']].apply(pd.to_numeric, errors = 'coerce')\n",
    "#df.column.astype(float)\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK FOR NEGATIVE VALUES BY COLORING NEGATIVE VALUES\n",
    "def color_negative_red(val):\n",
    "    color = 'red' if val < 38 else 'black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "test = df[['age','capital_gain']]\n",
    "colored = test.head().style.applymap(color_negative_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGING CATEGORICAL TO CONTINUOUS DATA\n",
    "def gender_to_numeric(x):\n",
    "    if x == 'Male':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying a Python list comprehension to the Pandas' data\n",
    "y = df.groupby('occupation')\n",
    "x = [el for el in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Classification?) Check dataset balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the data is imbalanced we need to balance it by creating new records \n",
    "#or reducing the number of records of the dependent variable\n",
    "\n",
    "#Creating the under sampling data \n",
    "print(X.shape, y.shape)\n",
    "\n",
    "####WAY TO AUTOMATICALY UPSAMPLE OR DOWNSAMPLE#####\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "os = RandomOverSampler(ratio = 1) #ratio will sample so that the ration of one feature and another is 50% and 50% respectively\n",
    "X_train_res, y_train_res = os.fit_sample(X, y)\n",
    "\n",
    "###################################################\n",
    "\n",
    "# #WAY TO DOWNSAMPLE (REDUCE NUMBER OF RECORDS)\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "# nm = NearMiss(random_state = 42)\n",
    "# X_res, y_res = nm.fit_sample(X, y)\n",
    "\n",
    "# #WAY TO UPSAMPLE (INCREASE NUMBER OF RECORDS)\n",
    "# from imblearn.combine import SMOTETomek\n",
    "# smk = SMOTETomek(random_state = 42)\n",
    "# X_res, y_res = smk.fit_sample(X, y)\n",
    "\n",
    "\n",
    "print(X_res.shape, y_res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/\n",
    "\n",
    "#CAN BE DONE FASTER WITH PANDAS DUMMIES)))#######\n",
    "df = pd.get_dummies(df, columns = ['col1', 'col2'])\n",
    "#################################################\n",
    "\n",
    "#Label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['new_col'] = le.fit_transform(df['Geography'])\n",
    "\n",
    "# Loop to encode string input values as integers for multiple columns\n",
    "features = []\n",
    "for i in range(0, X.shape[1]):\n",
    "    label_encoder = LabelEncoder()\n",
    "    feature = label_encoder.fit_transform(X[:,i])\n",
    "    features.append(feature)\n",
    "encoded_x = numpy.array(features)\n",
    "encoded_x = encoded_x.reshape(X.shape[0], X.shape[1])\n",
    "\n",
    "#One hot encoding for categorical information\n",
    "#We can one hot encode each feature after we have label encoded it.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "df['new_col'] = enc.fit_transform(df['Geography'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using SickitLearn to transform mixed type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data from Titanic dataset.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "\n",
    "#We create the preprocessing pipelines for both numeric and categorical data\n",
    "numeric_features = ['age','fare']\n",
    "\n",
    "numeric_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "#Append classifier to preprocessing pipeline\n",
    "#Now we have a full prediction pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "X = df.drop('survived', axis = 1)\n",
    "y = df['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format to load data into X and y for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLES ON HOW TO LOAD DATA IN X AND Y for the predictive model\n",
    "\n",
    "#Example 1\n",
    "X = np.array(df.drop(['Bankruptcy'], 1))\n",
    "y = np.array(df['Bankruptcy'])\n",
    "\n",
    "#Example 2\n",
    "X = df.iloc[:,[2,3]].values\n",
    "y = df.iloc[:,4].values\n",
    "\n",
    "#Turning dataframe into array of values\n",
    "dataset = df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format to get predictions from new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a df out of the array of data\n",
    "new_df = pd.DataFrame([[6, 168, 72, 35, 0, 43.6, 0.627, 65]])\n",
    "# We scale those values like the others\n",
    "new_df_scaled = scaler.transform(new_df)\n",
    "# We predict the outcome\n",
    "prediction = svc.predict(new_df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Sickit Learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASIC SICKIT LEARN LINEAR REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as LinReg\n",
    "x = df.index.values.reshape(-1, 1)\n",
    "y = df.education_num.values\n",
    "\n",
    "reg = LinReg()\n",
    "reg.fit(x,y)\n",
    "y_preds = reg.predict(x)\n",
    "print(reg.score(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(x = x, y = y_preds, c = 'b')\n",
    "plt.scatter(x = x, y = y, c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style = \"ticks\")\n",
    "# creates FacetGrid\n",
    "g = sns.FacetGrid(df, col = \"education_num\", height = 25)\n",
    "g.map(plt.hist, \"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIMENSIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DIMENSIONALITY REDUCTION\n",
    "\n",
    "https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7ehttps://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e\n",
    "\n",
    "#Feature selection is reducing number of vars among existing vars. \n",
    "\n",
    "\n",
    "Univariate feature selection examines each feature individually to determine the strength of the relationship of the feature with the response variable.\n",
    "\n",
    "\n",
    "Feature engineering is manually generating new features from existing features\n",
    "#Curse of diminesionality - more dimensions need more processing power, time, increases overfitting, limits number of algos to be used\n",
    "\n",
    "To reduce dimensions:\n",
    "\n",
    "\n",
    "#HEATMAPS to see corr\n",
    "We will only select features which have a correlation of above 0.5 (taking absolute value) with the output variable.\n",
    "\n",
    "\n",
    "\n",
    "LINEAR DIM REDUCTION\n",
    "\n",
    "1) PCA - used for dimensionality reduction in continuous data, // PCA rotates and projects data along the direction of increasing variance. The features with the maximum variance are the principal components.\n",
    "\n",
    "2) Factor analysis - values are expressed as functions and Gaussian noise is added.\n",
    "\n",
    "3) LDA - projects data in a way in which the class separability is maximised\n",
    "\n",
    "\n",
    "NON-LINEAR DIM REDUCTION (aka MANIFOLD LEARNING METHODS)\n",
    "\n",
    "Manifold hypothesis: in a high dimensional structure, most relevant information is concentrated in small number of low dimensional manifolds. \n",
    "\n",
    "1) Multi-dimensional scaling (MDS) Projects data to a lower dimension such that data points that are close to each other (in terms of Euclidean distance) in the higher dimension are close in the lower dimension as well.\n",
    "\n",
    "\n",
    "2) Isometric Feature Mapping (Isomap) : Projects data to a lower dimension while preserving the geodesic distance (rather than Euclidean distance as in MDS). Geodesic distance is the shortest distance between two points on a curve.\n",
    "\n",
    "\n",
    "3) Locally Linear Embedding (LLE): Recovers global non-linear structure from linear fits. Each local patch of the manifold can be written as a linear, weighted sum of its neighbours given enough data.\n",
    "\n",
    "4) Hessian Eigenmapping (HLLE): Projects data to a lower dimension while preserving the local neighbourhood like LLE but uses the Hessian operator to better achieve this result and hence the name.\n",
    "    \n",
    "5) Spectral Embedding (Laplacian Eigenmaps): mapping nearby inputs to nearby outputs. It preserves locality rather than local linearity\n",
    "\n",
    "\n",
    "6) t-distributed Stochastic Neighbor Embedding (t-SNE): Computes the probability that pairs of data points in the high-dimensional space are related and then chooses a low-dimensional embedding which produce a similar distribution.\n",
    "\n",
    "#F-Test checks for and only captures linear relationships between features and labels. A highly correlated feature is given higher score and less correlated features are given lower score. Correlation is highly deceptive as it doesn’t capture strong non-linear relationships. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.\n",
    "\n",
    "\n",
    "\n",
    "Auto-encoders\n",
    "\n",
    "Another popular dimensionality reduction method that gives spectacular results are auto-encoders, a type of artificial neural network that aims to copy their inputs to their outputs. They compress the input into a latent-space representation, and then reconstructs the output from this representation. An autoencoder is composed of two parts :\n",
    "\n",
    "    Encoder: compresses the input into a latent-space representation.\n",
    "    Decoder: reconstruct the input from the latent space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE OF DIM REDUCTION WITH PCA\n",
    "#We first scale the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "scaled_data = scaler.transform(df)\n",
    "#scaled_data.shape\n",
    "\n",
    "#Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2) #2 is the number of dimension we want\n",
    "pca.fit(scaled_data)\n",
    "x_pca = pca.transform(scaled_data)\n",
    "#scaled_data.shape\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(x_pca[:,0], x_pca[:,1],c=df['target_col'])\n",
    "plt.xlabel('First principle component')\n",
    "plt.ylabel('Second principle component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FEATURE SELECTION - FILTER METHOD\n",
    "##### WARNING: TO BE USED MAINLY ON SMALL DATASETS\n",
    "\n",
    "https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "\n",
    "#Loading the dataset\n",
    "x = load_boston()\n",
    "df = pd.DataFrame(x.data, columns = x.feature_names)\n",
    "df[\"MEDV\"] = x.target\n",
    "X = df.drop(\"MEDV\",1)   #Feature Matrix\n",
    "y = df[\"MEDV\"]          #Target Variable\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = df.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#FEATURE SELECTION BASED ON CORRELATION\n",
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"MEDV\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features\n",
    "\n",
    "#WE MUST FURTHER ELIMINATE FEATURES CORRELATED WITH EACH OTHER (FOR LINEAR REG)\n",
    "print(df[[\"LSTAT\",\"PTRATIO\"]].corr())\n",
    "print(df[[\"RM\",\"LSTAT\"]].corr())\n",
    "'''From the above code, it is seen that the variables RM and LSTAT are highly correlated with each other (-0.613808). \n",
    "Hence we would keep only one variable and drop the other. \n",
    "We will keep LSTAT since its correlation with MEDV is higher than that of RM.\n",
    "After dropping RM, we are left with two feature, LSTAT and PTRATIO. \n",
    "These are the final features given by Pearson correlation.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FEATURE SELECTION - WRAPPER METHOD - STATSMODELS(SM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BACKWARD ELIMINATION\n",
    "'''we feed all the possible features to the model, then iteratively remove \n",
    "the worst performing features one by one\n",
    "Metric to evaluate feature performance is pvalue above 0.05 (to keep the feature).'''\n",
    "\n",
    "#FORWARD\n",
    "'''we feed the features to a Machine Learning algorithm one by one.'''\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "#Adding constant column of ones, mandatory for sm.OLS model (OLS= Ordinary Least Squares)\n",
    "X_1 = sm.add_constant(X)\n",
    "\n",
    "#Fitting sm.OLS model\n",
    "model = sm.OLS(y, X_1).fit()\n",
    "print('P-values are: ', model.pvalues)\n",
    "\n",
    "#Backward Elimination\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "selected_features_BE = cols\n",
    "print()\n",
    "print('Selected features after loop are: ', selected_features_BE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FEATURE SELECTION - WRAPPER METHOD - RFE (RECURSIVE FEATURE ELIMINATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursively removing attributes and building a model on those attributes that remain\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, 7)\n",
    "\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)  \n",
    "\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now we need to find the optimum number of features, for which the accuracy is the highest. \n",
    "We do that by using loop starting with 1 feature and going up to 13. \n",
    "We then take the one for which the accuracy is highest.'''\n",
    "\n",
    "#no of features\n",
    "nof_list=np.arange(1,13)            \n",
    "high_score=0\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As seen from above code, the optimum number of features is 10. \n",
    "'''We now feed 10 as number of features to RFE \n",
    "and get the final set of features given by RFE:'''\n",
    "\n",
    "cols = list(X.columns)\n",
    "model = LinearRegression()\n",
    "\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, 10)             \n",
    "\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)  \n",
    "\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)              \n",
    "temp = pd.Series(rfe.support_,index = cols)\n",
    "selected_features_rfe = temp[temp==True].index\n",
    "print(selected_features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FEATURE SELECTION - EMBEDDED METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We iterate over the model training process and carefully extract those features \n",
    "which contribute the most to the training for a particular iteration, \n",
    "penalizing a feature given a threshold.\n",
    "Here we will do feature selection using Lasso regularization. \n",
    "If the feature is irrelevant, lasso penalizes its coefficient and make it 0. \n",
    "Features with coefficient = 0 are removed.'''\n",
    "\n",
    "reg = LassoCV()\n",
    "reg.fit(X, y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\n",
    "coef = pd.Series(reg.coef_, index = X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "imp_coef = coef.sort_values()\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\")\n",
    "\n",
    "#Here Lasso model has taken all the features except NOX, CHAS and INDUS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter method is less accurate. It is great while doing EDA, it can also be used for checking multi co-linearity in data.\n",
    "\n",
    "\n",
    "## Wrapper and Embedded methods give more accurate results but as they are computationally expensive, these method are suited when you have fewer features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE OF USING SICKIT LEARN TO SELECT THE TWO BEST FEATURES\n",
    "#https://github.com/knathanieltucker/bit-of-data-science-and-scikit-learn/blob/master/notebooks/FeatureSelection.ipynb\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X.shape\n",
    "(150, 4)\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y) #select top features (2 in this case)\n",
    "X_new.shape\n",
    "#You can also use print(X_new.nlargest(10, 'Score')) to see top (e.g.10) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SELECTION USING EXTRA TREES CLASSIFIER\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_)\n",
    "feat_importances = pd.series(model.feature_importances_, index = X.columns)\n",
    "feat_importances.nlargest(10).plot(kind = 'barh')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RECURSIVE FEATURE ELIMINATION (using Random Forest)\n",
    "'''Assigns weights to features (e.g., the coefficients of a linear model). \n",
    "Features whose weights are the smallest are pruned out.'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "m = RFECV(RandomForestClassifier(), scoring = 'accuracy')\n",
    "m.fit(X,y)\n",
    "m.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SELECTION USING SelectFromModel AND SVC\n",
    "\n",
    "'''features are considered unimportant and removed, if the corresponding coef_ or featureimportances values \n",
    "are below the provided threshold parameter.\n",
    "Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”.'''\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "m = SelectFromModel(LinearSVC(C=0.01, penalty='l1', dual=False))\n",
    "m.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SELECTION USING SelectFromModel AND LASSO\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "m = SelectFromModel(LassoCV(cv = 5))\n",
    "m.fit(X,y)\n",
    "#m.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SELECTION USING SelectFromModel AND DECISION TREE\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn import tree\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "fsel = ske.ExtraTreesClassifier().fit(X, y)\n",
    "model = SelectFromModel(fsel, prefit = True)\n",
    "X_new = model.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique to prun decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRUNNING DECISION TREES\n",
    "#https://stackoverflow.com/questions/49428469/pruning-decision-trees\n",
    "'''Directly restricting the lowest value (number of occurences of a particular class) of a leaf\n",
    "cannot be done with min_impurity_decrease or any other built-in stopping criteria.\n",
    "I think the only way you can accomplish this without changing the source code \n",
    "of scikit-learn is to post-prune your tree. \n",
    "To accomplish this, you can just traverse the tree \n",
    "and remove all children of the nodes with minimum class count less than 5 \n",
    "(or any other condition you can think of). I will continue your example:'''\n",
    "\n",
    "from sklearn.tree._tree import TREE_LEAF\n",
    "\n",
    "def prune_index(inner_tree, index, threshold):\n",
    "    if inner_tree.value[index].min() < threshold:\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "    # if there are children, visit them as well\n",
    "    if inner_tree.children_left[index] != TREE_LEAF:\n",
    "        prune_index(inner_tree, inner_tree.children_left[index], threshold)\n",
    "        prune_index(inner_tree, inner_tree.children_right[index], threshold)\n",
    "\n",
    "print(sum(dt.tree_.children_left < 0))\n",
    "# start pruning from the root\n",
    "prune_index(dt.tree_, 0, 5)\n",
    "sum(dt.tree_.children_left < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://www.andreagrandi.it/2018/04/14/machine-learning-pima-indians-diabetes/\n",
    "# Apply a scaler\n",
    "from sklearn.preprocessing import MinMaxScaler as Scaler\n",
    "\n",
    "scaler = Scaler()\n",
    "scaler.fit(train_set)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Applying multipe algorithms to classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.andreagrandi.it/2018/04/14/machine-learning-pima-indians-diabetes/\n",
    "# Import all the algorithms we want to test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Prepare an array with all the algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVC', SVC()))\n",
    "models.append(('LSVC', LinearSVC()))\n",
    "models.append(('RFC', RandomForestClassifier()))\n",
    "models.append(('DTR', DecisionTreeRegressor()))\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the configuration to run the test\n",
    "seed = 7\n",
    "results = []\n",
    "names = []\n",
    "X = train_set_scaled\n",
    "Y = train_set_labels\n",
    "\n",
    "# Every algorithm is tested and results are\n",
    "# collected and printed\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(\n",
    "        n_splits=3, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(\n",
    "        model, X, Y, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (\n",
    "        name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Finding best parameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.andreagrandi.it/2018/04/14/machine-learning-pima-indians-diabetes/\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1.0, 10.0, 50.0],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'shrinking': [True, False],\n",
    "    'gamma': ['auto', 1, 0.1],\n",
    "    'coef0': [0.0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "model_svc = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model_svc, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(train_set_scaled, train_set_labels)\n",
    "\n",
    "# Print the bext score found\n",
    "grid_search.best_score_\n",
    "\n",
    "\n",
    "# Create an instance of the algorithm using parameters\n",
    "# from best_estimator_ property\n",
    "svc = grid_search.best_estimator_\n",
    "\n",
    "# Use the whole dataset to train the model\n",
    "X = np.append(train_set_scaled, test_set_scaled, axis=0)\n",
    "Y = np.append(train_set_labels, test_set_labels, axis=0)\n",
    "\n",
    "# Train the model\n",
    "svc.fit(X, Y)\n",
    "\n",
    "\n",
    "#MAKING A PREDICTION\n",
    "# We create a new (fake) person having the three most correated values high\n",
    "new_df = pd.DataFrame([[6, 168, 72, 35, 0, 43.6, 0.627, 65]])\n",
    "# We scale those values like the others\n",
    "new_df_scaled = scaler.transform(new_df)\n",
    "\n",
    "# We predict the outcome\n",
    "prediction = svc.predict(new_df_scaled)\n",
    "\n",
    "\n",
    "# A value of \"1\" means that this person is likley to have type 2 diabetes\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using prediction pipeline with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10, iid=False)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print((\"best logistic regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADIENT DESCENT = iterative technique to minimize complex loss function\n",
    "'''Two ways to combat overfitting:\n",
    "1. Use more training data. The more you have, the harder it is to overfit\n",
    "the data by learning too much from any single training example.\n",
    "2. Use regularization. Add in a penalty in the loss function for building\n",
    "a model that assigns too much explanatory power to any one feature or\n",
    "allows too many features to be taken into account.'''\n",
    "\n",
    "'''Regularization loss: how much we penalize the model \n",
    "for having large parameters that heavily weight certain features'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLTK text classification / pos-neg sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 99,   0],\n",
       "       [100,   1]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Natural language processing. classification: pos or neg\n",
    "#Sourcce: https://www.youtube.com/watch?v=9VK4lWuD45U&list=PL2iM-fIRjbTBFazzQ5uEzeASpmP8o40y1&index=51\n",
    "#https://github.com/akky2892/Machine-Learning-The-future/blob/master/Natural%20Language%20Processing/Natural%20Language%20Processing.py\n",
    "'''\n",
    "Natural language processing\n",
    "'''\n",
    "\n",
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importing the dataset\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/akky2892/Machine-Learning-The-future/master/Natural%20Language%20Processing/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "#Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^A-Za-z]', '', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "\n",
    "#Creating a bag-of-words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values\n",
    "\n",
    "#Spliting into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#Fitting Naive Bayes to training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#Predicting test set result\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#Making confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Keras Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 3.6181 - acc: 0.6380\n",
      "Epoch 2/20\n",
      "768/768 [==============================] - 0s 90us/step - loss: 1.8647 - acc: 0.5599\n",
      "Epoch 3/20\n",
      "768/768 [==============================] - 0s 102us/step - loss: 1.3050 - acc: 0.5534\n",
      "Epoch 4/20\n",
      "768/768 [==============================] - 0s 110us/step - loss: 1.0270 - acc: 0.5456\n",
      "Epoch 5/20\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.9345 - acc: 0.5326\n",
      "Epoch 6/20\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.8688 - acc: 0.5260\n",
      "Epoch 7/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.8265 - acc: 0.5130\n",
      "Epoch 8/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.7864 - acc: 0.5313\n",
      "Epoch 9/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.7502 - acc: 0.6185\n",
      "Epoch 10/20\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.6880 - acc: 0.6615\n",
      "Epoch 11/20\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.6715 - acc: 0.6667\n",
      "Epoch 12/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6542 - acc: 0.6719\n",
      "Epoch 13/20\n",
      "768/768 [==============================] - 0s 142us/step - loss: 0.6459 - acc: 0.6523\n",
      "Epoch 14/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6415 - acc: 0.6549\n",
      "Epoch 15/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6389 - acc: 0.6654\n",
      "Epoch 16/20\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.6329 - acc: 0.6563\n",
      "Epoch 17/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6254 - acc: 0.6510\n",
      "Epoch 18/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6216 - acc: 0.6563\n",
      "Epoch 19/20\n",
      "768/768 [==============================] - 0s 128us/step - loss: 0.6182 - acc: 0.6589\n",
      "Epoch 20/20\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.6172 - acc: 0.6680\n",
      "768/768 [==============================] - 0s 514us/step\n",
      "Accuracy: 66.41\n"
     ]
    }
   ],
   "source": [
    "#Basic Keras neural network\n",
    "#Source: https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "# load the dataset\n",
    "dataset = loadtxt(r'D:\\Downloads\\DATASETS\\general\\pima_indian_dataset.csv', delimiter=',')\n",
    "\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=20, batch_size=40)\n",
    "\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
