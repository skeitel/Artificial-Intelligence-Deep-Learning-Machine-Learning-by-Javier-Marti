#dataset downloaded from https://www.kaggle.com/yelp-dataset/yelp-dataset/version/7import json as jimport pandas as pdimport reimport numpy as npfrom nltk.corpus import stopwordsfrom nltk.stem import SnowballStemmerfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.svm import LinearSVCfrom sklearn.pipeline import Pipelinefrom sklearn.model_selection import train_test_splitfrom sklearn.feature_selection import SelectKBest, chi2#we import the json data and create Panda's dataframejson_data = Nonewith open ('yelp_academic_dataset_review.json', encoding="utf8")as data_file:    lines = [next(data_file) for x in range(100000)] #WARNING with this line we are limiting the number of rows we read from the file, since the file contains 1.5 million rows...4GB of data that can crash PCs memory#print(lines)    #lines = data_file.readlines()#[:10]    #print(lines)joined_lines = '[' + ','.join(lines) + ']'json_data = j.loads(joined_lines)data = pd.DataFrame(json_data)#print(joined_lines)print('PHASE 1 LOADING DATA COMPLETE...')#instantiate stopwordsstemmer = SnowballStemmer('english')words = stopwords.words('english') #this loads just a list of words#create new column with cleaner datadata['cleaned'] = data['text'].apply(lambda x: ' '.join([stemmer.stem(i) for i in re.sub('[^a-zA-Z]',' ',x).split() if i not in words]).lower()) #join and stem every text blob (if not in stop words) then split and lower#train split at 20%X_train, X_test, y_train, y_test = train_test_split(data['cleaned'], data.stars, test_size= 0.2)#create pipeline. K value may be "all" or a finite value inferior to the number of recordspipeline = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2), stop_words='english', sublinear_tf=True)),                      ('chi', SelectKBest(chi2, k='all')),                      ('clf', LinearSVC(C=1.0, penalty='l1', max_iter=3000, dual=False))])print('PHASE 2 BUILDING PIPELINE COMPLETE...')#fit model and instantiate vectorizersmodel = pipeline.fit(X_train, y_train)vectorizer = model.named_steps['vect']chi = model.named_steps['chi']clf = model.named_steps['clf']feature_names = vectorizer.get_feature_names()feature_names = [feature_names[i] for i in chi.get_support(indices=True)]feature_names = np.asarray(feature_names)print('PHASE 3 COMPLETE, ABOUT TO LOOP AND PRINT RESULTS...')#Print most popular words and accuracy scoretarget_names = ['1','2','3','4','5']print('top 10 keywords for class:')for i, label in enumerate(target_names):    top10 = np.argsort(clf.coef_[i])[-10:]    print('{}:{}'.format(label, ' '.join(feature_names[top10])))print('accuracy score: ' + str(model.score(X_test, y_test)))#Predict new text# text_to_predict = 'that was an awesome place. Great food'text_to_predict = 'Horrible food and I did not like the place. Not recommended'new_prediction = model.predict([text_to_predict])print('The prediction stars punctuation for the text "' + text_to_predict + '" is:')print(new_prediction, 'stars')